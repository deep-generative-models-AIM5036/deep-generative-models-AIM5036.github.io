# Model Structure

![VQVAE1](https://user-images.githubusercontent.com/60708119/200171694-c35e6a49-f222-41e0-9f9e-9cfe8bf060cd.png)  
- n : batch size
- h : image height
- w : image width
- c : number of channels on the input image
- d : number of channels in the hidden state

# Processing
![VQVAE2](https://user-images.githubusercontent.com/60708119/200171775-52cf265a-8534-4f71-97e0-24db6cc4a9cf.png)  
1. **Reshape** : all dimensions except the last one are combined into one so that we habe n×h×w vectors each if dimensionality d
2. **Calculating distances** : for each of the n×h×w vectors we calculate distance from each of k vectors of the embedding dictionary to obtain a matrix of shape (n×h×w, k)
3. **Argmin** : for each of the n×h×w vectors we find the index of closest of the k vectors from dictionary
4. **Index from dictionary** : index the closest vector from the dictionary for each of n×h×w vectors
5. **Reshape** : convert back to shape (n, h, w ,d)
6. **Copying gradients** : If you followed up till now you'd realize that it's not possible to train this architecture through backpropagation as the gradient won't flow through argmin. Hence we try to approximate by copying the gradients from z_q back to z_e. In this way we're not actually minimizing the loss function but are still able to pass some information back for training.

# Vector Quantisation
The posterior categorical distribution q(z|x) probabilities are defined as one-hot as follows  
<img width="792" alt="VQVAE3" src="https://user-images.githubusercontent.com/60708119/200172372-f21bb15c-798b-4a9b-89cd-e20139ce6844.png">  
- $z_e(x)$ is the output of the encoder network
- IN VAE, $ log\ p(x) $ can bound with ELBO, VQ-VAE proposal distribution $q(z=k|x)$ is deterministic
- By defining a simple uniform prior over z we obtain a KL divergence constant and equal to log K

<img width="785" alt="VQVAE4" src="https://user-images.githubusercontent.com/60708119/200172375-26896a4f-a80e-446f-a17b-de7ed2c55688.png"> 

- The representation $z_e(x)$ is passed through the discretizaion bottle neck followed by mapping onto the nearest element of embedding $e$ as given in equation 1 and 2.

<img width="669" alt="VQVAE5" src="https://user-images.githubusercontent.com/60708119/200172632-d6109327-d959-4e75-90c4-91cdc40a769a.png">    

- 모델은 입력 x로부터 받아 인코더를 통해 출력 $z_e(x)$를 생성함
- Discrete latent variable z는 shared embedding space인 e로부터 가장 가까운 neighborhoodd와의 distance 계산에 의해 결정됨. 즉 가장 가까운 vector로 결정
- e인 embedding space를 codebook으로 부르며, $e\ \in\ R^{K×D}$ 이며, 여기에서 K는 discrete latent space의 size가 됨(즉, K-way categorical)
- D는 각각의 latent embedding vector인 $e_i$ 의 차원 수
- 종합적으로, K 개의 embedding vector가 존재하며, 이를 수식으로 나타내면 $e_i\ \in\ R^D,\ i\ \in\ 1, 2, ..., K$ 가 됨  

# Loss function  
> Total Loss  
  
$$ L = log\ p(x|z_q(x)) + ||sg[z_e(x)]-e||_2^2\ + \beta||z_e(x)-sg[e]||_2^2  $$  
- sg : stand for the stopgradient operator
- Decoder optimized the first loss term only
- Encoder optimizes the first and the last loss terms
- Embeddings are optimized by the middle loss term
 

##
- Since VQ-VAE assume a uniform prior for z, the KL term that usually appears in the ELBO is constant w.r.t. the encoder parameters and can thus be ignored for training  
> Reconstruction Loss 
 
 $$ L= log\ p(x|z_q(x)) $$
### Reconstruction Loss : which optimizes the decoder and encoder
$$ z_q(x) = e_k, \quad where \quad k=argmin_j||z_e(x)-e_j||_2 \qquad (2)  $$  
- There is no real gradient defined for equation 2
- Approximate the gradient similar to the staright-through estimator
- Just copy gradients from decoder input $z_q(x)$ to encoder output $z_e(x)$

> Codebook Loss

$$ L = ||sg[z_e(x)]-e||_2^2 $$

### Codebook Loss : due to the fact that gradients bypass the embedding, we use a dictionary learning algorithm which uses an L2 error to move the embedding vectors $e_i$ towards the encoder output
- This loss term is only used for updating the dictionary

> Commitment Loss
$$ L = \beta||z_e(x)-sg[e]||_2^2 $$
### Commitment Loss : since the volume of the embedding space is dimensionless, it can grow arbirtarily if the embeddings $e_i$ do not train as fast as the encoder parameters, and thus we add a commitment loss to make sure that the encoder commits to an embedding
- Author found the resulting algorithm to be quite rebust to $\beta$, as the results did not vary for values of $\beta$ ranging from 0.1 to 2.0
- VQ-VAE use $\beta$ = 0.25 in all experiments  
** In general this would depend in the scale of reconstruction loss  

# Log-likelihood of the complete model

$$ log\ p(x) = log\ \sum_k d=p(x|z_k)p(z_k) $$
- Because the decoder $p(x|z)$ for $z = z_q(x)$ from MAP-inference
- the decoder should not allocate any probability mass to $p(x|z)$ for $z \ne z_q(x)$ once it has fully converged  
-> $ log\ p(x) \approx log\ p(x|z_q(x))p(z_q(x))$  
-> $ log\ p(x) \ge log\ p(x|z_q(x))p(z_q(x))$  (Jensen's inequality)

# Posterior Collapse
## Why the posterior collapse occurs
- When the decoder can sufficiently generate only past data without latent z
- The hypothesized Gaussian prior has no information in fact
- Gap between ELBO and evidence, failure of true posterior approximation
- Because the encoder does not express mearning z at the beginning of training

<img width="330" alt="VQVAE6" src="https://user-images.githubusercontent.com/60708119/200174355-d6ecbe62-aee0-47d2-8b8a-50a763d39a4d.png">
  
The approximate posterior mimics the prior as it is, and the model means that learning proceeds while ignoring the latent variable.


# PixelCNN Prior
After training VQ-VAE, train PixelCNN over the discrete latents for images  
Discrete latents vector generated by pre-trained PixelCNN enters the VQ-VAE decoder input and generate images
