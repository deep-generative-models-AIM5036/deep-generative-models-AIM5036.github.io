---
layout: post
title: "WGAN"
date: 2022-11-12
author: Khan Osama, Jeonghyeon Kim
categories: GAN
tags: WGAN, GAN
use_math: true
---

# WGAN[작성중]

# Introduction

WGAN 이란?
적대적 신경 생성망 GAN의 비용함수를 Wasserstein distance로 설정하여 최적화를 진행하는 신경망이다.

기존의 GAN은 이미지가 복잡해 짐(고차원화) 에 따라 학습의 난이도가 아주 올라가게 되고 이로 인해
학습이 매우 불안정 해지는 문제가 발생하게 되었다.  
WGAN에서는 학습의 불안정성의 원인을 metric(거리)이라 보고 이것을 수학적으로 해결하고자 하였다.

![Untitled](/assets/WGAN/KL.png)  
기존의 두 분포 사이의 거리를 측정하기 위해 주로 쓰이는 KLD는 위수식과 같다.  
하지만 KLD와 같은 metric은 분모에 위치하는 $P_g(x) = 0$ 이고,
$P_r(x) \ne 0$ 인 곳이 발생하게 된다면 발산하게 되는 문제점이 생긴다.  
본 논문에서는 저차원에서 주로 이러한 문제가 발생한다고 서술하고 있다.

![Untitled](/assets/WGAN/long_distribution.png)  
또한 위 그림과 두 분포가 겹치지 않는 다면, 즉 support가 겹치지 않은 상황에서 역시
두 분포 사이의 거리가 상당히 멀어 좋지 않은 gradient feedback을 주게 되고
유의미한 학습이 일어나기가 힘들다 는 문제가 있다.

따라서 본 논문에서는 모델의 학습이 얌전히 수렴하고, 매끄러운 metric을 찾는 것을 목표로 한다.

# Different Distances

어떤 metric이 더 GAN의 학습에 적절한 한지 알아보기 위해 다양한
Distance 방법들을 살펴 본다.

## The Total Variation(TV) Distance

![Untitled](/assets/WGAN/TVD.png)  
위의 식은 Total Variation (TV) distance를 나타낸다. 여기서 $sup$ 은 supremum 최소상한라는 의미이며
어떤 집합의 상한 중 가장 작은 값을 의미한다.

TVD는 두확률 분포의 가능한 측정 값들 중 차이가 가장 큰 값 $sup$ 으로 정의된다.

![Untitled](/assets/WGAN/TVD_example.png)  
위 그림에서 처럼, 가능한 사건들을 A로 잡았을 때, 두 분포를 비교 했을떄 측정 값은
서로 다른값을 가지게 된다. 따라서 두 분포중 큰 값을 TVD로 한다.

![Untitled](/assets/WGAN/TVD_zero.png)
TVD 에서 만약 두 분포가 겹치지 않는다면 즉 support가 공집합이라면,
TVD는 1을 가지게 된다.

## The Kullback-Leibler Divergence

![Untitled](/assets/WGAN/KL.png)  
위에서 언급 했던 대로 KLD은 분모에 위치하는 $P_g(x) = 0$ 이고,
$P_r(x) \ne 0$ 인 곳이 발생하게 된다면 발산하게 되는 문제점이 생긴다.

## The Jensen-Shannon Divergence

![Untitled](/assets/WGAN/JSD.png)  
JSD는 KLD를 이용하여 간략하게 표현 될 수 있다.  
만약 두 분포의 support 가 겹치지 않는다면
![Untitled](/assets/WGAN/support.png)  
으로 나타나게 되고  
![Untitled](/assets/WGAN/JS_log2.png)  
위 수식처럼 $log2$ 로 두 분포사이의 거리가 고정된다.
따라서 JSD는 발산하지는 않지만 $log2$ 처럼 상수 값만 가지게 되므로
좋은 피드백을 주는 것이 어렵다.

## The Earth Mover (EM) Distance (Wasserstein-1)

![Untitled](/assets/WGAN/EM.png)  
위 식은 EMD 즉 Wasserstein 을 보여준다.  
EM은 $P_r$ 을 $P_g$ 로 변경하기 위한 여러 경우 중
가장 최적의 transform plan에 대한 cost를 구할 수 있도록 해준다.

# EM의 타당성 설명

![Untitled](/assets/WGAN/EM1.png)  
본 논문에서는 위에서 논의한 다양한 distance 들과 EM을 비교하여 EM의 타당성을 이야기한다.
기존의 GAN 논문에서 Disciminator 와 Gnerator가 학습할 떄 EM을 제외한 다른 metric을 사용한다면,  
$\theta \ne 0$ 일 경우 두 분포가 겹치지 않게 되고 무의미한 값들을 구하게 된다.  
하지만 EM의 경우 공식에 따라 두 점사이의 거리는 항상 $\left\vert \theta \right\vert$ 로 나타나게된다.

![Untitled](/assets/WGAN/EM_graph.png)  
위 그림은 논문에서 JSD와 EM의 gradient를 비교하여 보여주는 그림이다.  
오른쪽의 그림은 JSD를 보여주는데 JSD의 경우 $\theta = 0$ 이외에선 상수로 일정하게 나타나고,  
$0$ 일 떄는 $0$ 으로 나타나게 된다. 따라서 유의미한 gradient를 얻기가 힘든 것을 알 수 있다.  
왼쪽 첫번 째 그림의 EM의 경우 좀 더 유의미한 gradient를 내보내는 것을 볼 수 있다.
따라서 EM 즉 W distance가 GAN의 학습에 좀더 유용하다는 것을 알 수 있다.

# Wasserstein GAN

본 논문에서는 Wasserstein 1 Distance를 GAN의 Loss function으로 정의 했다.  
![Untitled](/assets/WGAN/ori_DM.png)  
의 EM 공식을 살펴보면 inf가 나오는 해당 부분의 계산이 intractable 하다.  
따라서 논문에서는 Kantorovich-Rubinstein duality를 사용하여 식을 변경한다.  
![Untitled](/assets/WGAN/W_ch.png)  
이렇게 식을 정리하게 되면 위 수식과 같이된다.  
위 수식에서 $\left\vert\left\vert f \right\vert\right\vert_L \leq 1$은 $f$ 가 1-립쉬츠 함수,  
즉 임의의 두점 사이의 평균변화율이 1이 넘지 않는 함수 라는 뜻을 의미한다.  
위의 정리된 수식을 사용하면 $f$ 만 알아 낼 수 있다면 $W$ 값을 얻을 수 있다.

본 논문에서는 $w$를 추가로 변수로 사용하여 $f_w$ 를 업데이트 하고 있다.  
기존 위의 수식에서 $P_\theta$ 를 $g\theta(z)$ 로 바꾸어 주면 아래 수식과 같은 식이 나온다.

![Untitled](/assets/WGAN/DM_ch2.png)  
위의 수식의 모양이 GAN의 Loss 함수와 비슷한 모습을 보이는 것을 알 수 있다.

본 논문에선 립시츠 조건을 만족하도록 discriminator (Critic)의 파라미터 W가 일정범위 [-0.01, 0.01] 내에
있도록 강제로 제한한다.  
이과정을 clipping이라고 부른다. 이 방법에 대해서 논문에선 cleary terrible way 라고할 정도로
좋지 않게 표현하고 있다. 하지만 실험 결과 해당 방법이 너무 간결하고 좋은 결과를 보여 사용했다고
언급하고 있다.

![Untitled](/assets/WGAN/%EC%B0%A8%EC%9D%B4%EC%A0%90GAN.png)  
위 그림에서는 기존 GAN과 WGAN의 discriminator (Critic)이 어떻게 다르게 동작하는지 보여준다.  
그림에서 빨간색은 GAN의 discriminator를 나타내는데, 빠르게 fake와 real을 잘 구분할 수 있게 되지만
discriminator가 결국 gradient vanishing이 발생하게 된다.  
하지만 하늘색 선의 WGAN의 경우 critic이 gradient vanishing에 빠지지 않고 linear 하게 gradient가 잘 발생된다.  
따라서 model collapse가 발생할 수 없는 상황이 된다.

# Empirical Results

![Untitled](/assets/WGAN/result_DM.png)
위 그림은 EMD를 사용하여 학습을 진행할 때 나오는 이미지와 estimation 간의 상관관계를 보여준다.  
그림을 살펴보면 W 값은 이미지의 퀄리티가 좋아 질 수록 W가 점점 감소하는 것을 볼수 있고, 마지막
사진에선 망가진 이미지가 나올 때는 W가 줄어들지 않는 모습을 보인다.

![Untitled](/assets/WGAN/result_JS.png)
위 그림은 JSD를 사용하여 학습을 진행항 때 나오는 이미지와 estimation 간의 상관관계를 보여준다.  
JSD의 같은 경우 이미지의 품질에 상관없이 estimation이 $log2$ 즉 약 0.69로 고정되는 모습을 보인다.  
또한 마지막 망가진 이미지의 사진에선 오히려 내려갔다 올라가는 모습을 보이기도한다.

# Conclusion

해당 논문에서는 dicriminator와 Generator 간의 균형을 유지하며 학습하기 어려운  
GAN이 가진 문제점을 해결하기 위해 Discriminator 대신 새로운 Critic을 정의하여 사용하며,
Critc을 구하기 위해 Wasserstein distance (EMD)를 사용합니다.
