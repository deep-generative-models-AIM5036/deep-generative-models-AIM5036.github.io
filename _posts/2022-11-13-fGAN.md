---
layout: post
title: "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization"
date: 2022-11-13
author: Janghun Kim, Hyogeun Byun
categories: GAN
tags: [paper review, Neurips, GAN]
use_math: true
published: true
---

# *f*-GAN
Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. "f-gan: Training generative neural samplers using variational divergence minimization." Advances in neural information processing systems 29 (2016).

## Introduction

확률적 생성모델은 주어진 domain $\chi$ 상의 확률분포를 서술한다. 

가능한 모델 집합 $Q$에서 생성모델 Q가 주어졌을 때 우리는 일반적으로 다음에 관심이 있다:
- Sampling
- Estimation
- Point-wise likelihood estimation

GAN은 정확한 sampling과 근사추정이 가능한 인상적인 모델이다. 여기서 사용된 모델은 균등분포 같은 random input vector를 받는 feedforward 신경망이다. 

이런 확률적 feedforward 신경망을 generative neural samplers라고 부를 것이다. 

original GAN에서, neural sample를 JSD의 근사적 최소화로 추정하는 것이 가능함이 증명되어 있다. 

$$ D_{JS}(P \| Q) = {1 \over 2} D_{KL}(P \| {1 \over 2}(P+Q)) + {1 \over 2} D_{KL}(Q \| {1 \over 2}(P+Q)) $$

$D_{KL}$은 Kullback–Leibler divergence이다.

GAN 학습의 중요한 테크닉은 동시에 최적화된 Discriminator 신경망을 만든 것에 있는데, 우리는 이 논문에서 GAN 학습목적(training objectives)과 이를 임의의 *f-divergence*로 일반화하고자, GAN을 variational divergence 추정 framework로 확장할 것이다.

구체적으로, 이 논문에서 보여주는 것은:

- GAN 학습목적을 모든 *f*-divergence에 대해 유도하고 여러 divergence 함수를 소개한다: Kullback-Leibler와 Pearson Divergence를 포함한다.
- 우리는 GAN의 saddle-point 최적화를 단순화할 것이고 또 이론적으로 증명한다.
- 자연 이미지에 대한 generative neural sampler을 측정하는 데 어느 divergence 함수가 적당한지 실험적 결과를 제시한다.

---

## Method

먼저 divergence 추정 framework를 리뷰부터 하겠다. 이후 divergence 추정에서 model 추정으로 확장하겠다.

### The *f*-divergence Family

Kullback-Leibler divergence같이 잘 알려진 것은 두 확률분포 간 차이를 측정한다. 

두 분포 $P$와 $Q$가 있고, domain $\chi$에서 연속밀도함수 $p$와 $q$에 대해 *f-divergence*는  
$ f : \mathbb{R}_+ \rightarrow \mathbb{R} $이 $f(1)=0$인 convex하고 lower-semicontinuous한 함수 $f$에 대해
 
$$ D_f(P \Vert Q) = \int_{\chi} q(x) f \Bigl( {p(x) \over q(x)} \Bigr) dx $$

로 정의된다.


### Variational Estimation of *f*-divergences

*Nyugen* 등 연구자는 $P$와 $Q$로부터의 sample만 주어진 경우에서 *f*-divergence를 측정하는 일반적인 변분법을 유도했다. 우리는 이를 고정된 모델에서 그 parameter를 측정하는 것으로까지 확장할 것이고, 이를 *variational divergence minimization*(VDM)이라 부를 것이다. 또한 적대적 생성 학습법은 이 VDM의 특수한 경우임을 보인다.

모든 convex하고 lower-semicontinuous인 $f^\ast$ (*Fenchel conjugate*)를 갖는다. 이는

$$ f^\ast(t) = \quad sup \quad  \{ ut-f(u) \} \\ u \in dom_f \qquad $$

로 정의된다.

또한 $f^\ast$ 역시 convex하며 lower-semicontinuous이고 $f^{\ast\ast} = f$이므로 $ f(u) = sup_{t \in dom_{f^\ast}} \{ tu-f^\ast(t) \} $로 쓸 수 있다.

*Nguyen* 등 연구자는 lower bound를 구했다: $\tau$가 $T: \chi \rightarrow \mathbb{R} $인 함수들의 임의의 집합일 때, 

$$ D_f(P \Vert Q) \ge sup_{T \in \tau} (\mathbb{E}_{x \sim P} [T(x)] - \mathbb{E}_{x \sim Q} [f^\ast(T(x))]) $$

변분법을 취해서, 

$$ T^\ast(x) = f^{'} \Bigl( {p(x) \over q(x)} \Bigr)  $$

아래는 여러 *f*-divergence를 생성함수와 함께 나타낸 것이다.

![image](/assets/fGAN_img/01.PNG)

### Variational Divergence Minimization(VDM)

앞서 소개한 *f*-divergence $D_f(P\Vert Q)$에 대한 lower bound를 살펴보면 식이 상당히 GAN objective와 닮았다. 여기서부터는 마치 GAN과 같이 *Q*와 *T*를 neural network를 사용하여 모델링 할 수 있다.

이 때, $Q$는 random vector를 input으로 받아 우리가 원하는 sample을 내보내는 generative parametric model로 $Q_{\theta}$라 표현할 수 있고 $T$는 input으로 sample을 받아 scalar 값을 내보내는 variational function으로 $T_{\omega}$라 표현할 수 있다

$$ F(\theta, \omega) = \mathbb{E}_{x \sim P} [T_{\omega}(x)] - \mathbb{E}_{x \sim Q_{\theta}} [f^\ast({T_\omega}(x))] $$

여기에 다양한 종류의 f-divergence를 사용하여 껴넣으면 새로운 GAN objective가 만들어지는 것이다.
단, 앞서 *f*-divergence에 사용된 f에 대한 conjugate 함수 $f^\ast$를 사용하였기 때문에 variational function $T_{\omega}$의 domain이 $ dom_f \qquad $이 되도록 해야한다. 

이 수식을 조금 더 GAN 형태와 비슷하게 만들기 위해 함수를 함수 $T_{\omega}(x)$를 output activation function $g_f: \mathbb{R} \rightarrow dom_f \qquad$ 와 discriminator MLP에 해당할 $V_{\omega}: \chi \rightarrow \mathbb{R} $로 나누어 생각해보겠습니다:  $T_{\omega}(x)=g_f(V_{\omega}(x))$. 그러면 이제 아래와 같은 table에서 이에 맞게 activation function만 잘 조정해주면 원하는 divergence에 대해 새로운 GAN을 만들 수 있게 된다.

![image](/assets/fGAN_img/02.PNG)

GAN으로 따지면 *v* 값이 discriminator MLP인 $V_{\omega}(x)$에서 나온 값이라 생각하면 이해가 쉽다. 즉, $V_{\omega}$는 *x*가 실제 distribution에서 나왔을 확률을 보낸다고 생각하시면 된다. 위 그림에서 왼쪽 그래프는 실제 distribution에서 sample이 나왔을 때는 $g_f$ 값이 양수이지만 잘못 분류한 경우(*v* 값이 음수)에 대해서는 penalty를 주는 것을 확인할 수 있다. 오른쪽 그래프로부터는 반대인 경우이다. 

### Example: Univariate Mixture of Gaussians

물론 앞서 아름답게 논리를 전개해왔지만, 예측한 것과 같이 실제로도 하한 값이 잘 계산되는지 그리고 진짜 solution에 대한 obejective 값과 variational representation으로 얻은 값 사이의 차이가 얼마나 되는지 등을 확인해볼 필요가 있겠죠. 다양한 *f*-divergence들에 대한 objective 함수를 GAN 알고리즘으로 문제를 풀었을 때, 간단한 예제에 대해 결과를 확인해보면 다음과 같습니다:

![image](/assets/fGAN_img/03.PNG)

왼쪽 테이블을 보시면, lower bound인 $F(\theta \hat, \omega \hat)$ 값이 실제로 true parameter에 대한 objective 값인 $ D_f(P \Vert Q_{\theta^\ast}) $보다 약간 작습니다. 이 차이가 알고리즘을 사용하여 찾은 parameter와 실제 parameter의 차이와도 잘 대응됩니다.

그런데 사실 저는 이 부분보다는 오른쪽 테이블에 정리된 실험이 더 중요한 것 같습니다. 이 테이블은 학습(train)할 때는 특정 divergence로 학습을 시킨 후 generator인 $Q_{\theta}$를 고정시키고 새로운 divergence에 대해 $T_{\omega}$를 바꿔 discriminator를 재 학습(test)시킨 다음 true distribution에 대한 objective 값을 비교한 것입니다. 

---

## Algorithms for Variational Divergence Minimization(VDM)

이제 우리는 목적함수의 saddle point를 찾기 위한 수치적 방법을 논할 것이다.

1. Goodfellow가 제안한 교대(alternative) 학습 방법
2. 더 직접적인 single-step 최적화 과정

두 가지를 쓴다.

### Single-Step Gradient Method

원래 것과는 달리 inner loop가 없고, 단 하나의 back-propagation으로 $\omega$와 $\theta$의 gradient가 계산된다.

![image](/assets/fGAN_img/04.PNG)

saddle point 근방에서 $\theta$에 대해 볼록하고 $\omega$ 에 대해  오목한 $F$에 대해 위 알고리즘 1은 saddle point $(\theta^\ast, \omega^\ast)$에서 수렴함을 보일 수 있다.

이를 위해 다음 정리를 논문 부록에서 보이고 있다.

**Theorem 1.** $\pi^t := (\theta^t, \omega^t) $ 라 하고, 조금 위의 근방 조건을 만족하는 saddle point $ \pi^\ast = (\theta^\ast, \omega^\ast) $ 가 존재한다고 가정하자. 더욱이 위 근방에 포함되는 $ J(\pi) = {1\over 2} \Vert \nabla F(\pi) \Vert_2^2 $ 를 정의할 수 있고, $F$는 $ \pi^\ast $ 근방 모든 $ \pi, \pi^{'} $ 에 대해 $ \Vert \nabla J(\pi^{'}) - \nabla J(\pi) \Vert_2 \le L \Vert \pi^{'} - \pi \Vert_2 $ 를 만족하는 상수 $ L > 0 $ 가 존재할 수 있게 하는 $F$는 충분히 smooth하다.  
알고리즘 1에서 step-size를 $ \eta=\delta / L$ 라 할 때, 

$$ J(\pi^t) \le \Bigl( 1 - {\lambda^2 \over 2L} \Bigr)^t J(\pi^0) $$

를 얻을 수 있다.  
또 gradient $ \nabla F(x) $ 의 2차 norm은 기하적으로 감소한다.

### Practical Considerations

Goodfellow가 GAN 논문 당시 제안한 팁 중에 \\( \mathbb{E}\_{x \sim Q\_{\theta}} [log(1-D_\omega(x))]\\)를 최소화하는 대신 \\( \mathbb{E}\_{x \sim Q\_{\theta}} [log D\_\omega(x)] \\)를 최대화하는 것으로 속도를 빠르게 하는 것이 있었다.  
이를 더 일반적인 *f*-GAN에 적용하면 

$$ \theta^{t+1} = \theta^t + \eta \nabla_\theta \mathbb{E}_{x \sim Q_{\theta^t}} [g_f(V_{\omega^t}(x))] $$

그렇게 함으로써 generator 출력을 최대화할 수 있다.

실험적으로, 우리는 Adam과 gradient clipping이 LSUN 데이터셋의 대규모 실험에서는 특히 유용함을 발견하였다.

---

## 실험(Experiments)

이제 VDM에 기초하여 MNIST와 LSUN에 대해 학습시킨 결과는 다음과 같다.

![image](/assets/fGAN_img/05.PNG)

![image](/assets/fGAN_img/06.PNG)
