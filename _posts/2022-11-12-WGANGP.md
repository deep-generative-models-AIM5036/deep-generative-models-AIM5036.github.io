---
layout: post
title:  "WGAN-GP"
date:   2022-11-12
author: SeungHoo Hong
categories: GAN
tags: WGAN-GP, GAN
use_math: true
---

# WGAN-GP

Improved Training of Wasserstein GANs

![Untitled](/assets/WGAN-GP_img/Untitled.png)

# Background

## GAN

GANì€ Generatorì™€ Discriminatorì˜ minmaxê²½ìŸì—ì„œ Disciminatorê°€ ë§¤ stepì—ì„œ optimal ì¼ ë•Œ ê²½ìŸì€ ìˆ˜ë ´í•˜ë©° ì´ë•Œ Generatorê°€ ìƒì„±í•˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ëŠ” training dataì˜ ë¶„í¬ì™€ ë™ì¼í•˜ê²Œ ë¨ì„ ì¦ëª…í•˜ë©´ì„œ íŠ¹ì • ëª¨ë¸ êµ¬ì¡°ì— ì œì•½ì„ ë°›ì§€ ì•ŠëŠ” ê°•ë ¥í•œ ìƒì„±ëª¨ë¸ì˜ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” VAEì™€ normalize flow ëª¨ë¸ì´ architecture ìƒì˜ ì œì•½ ì¡°ê±´(flowëª¨ë¸ì˜ ê²½ìš° ì—­í•¨ìˆ˜ê°€ ì¡´ì¬í•´ì•¼í•˜ë©°, VAEì˜ ê²½ìš° samplingì—ì„œ ììœ ë¡œìš¸ ìˆ˜ ì—†ë‹¤)ì´ ì¡´ì¬í•˜ëŠ” ê²ƒê³¼ ëŒ€ë¹„ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

GAN ë…¼ë¬¸ì—ì„œëŠ” optimal Discriminatorì´ ì¡´ì¬í•˜ë©° $ D^{*}_G(x)=\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)} $ ì´ê³  $ D^{*}_{G} $ ì´ ì£¼ì–´ì¡Œì„ ë•Œ Generatorì˜ ìµœì ê°’ì„ ì°¾ëŠ” ê²ƒì€ $ p_{data} $ ì™€ $ p_{g} $ ì˜ Jensen-Shannon Divergenceì„ ê°ì†Œì‹œí‚¤ëŠ” $ p_{g} $ ë¥¼ ì°¾ëŠ” ê²ƒê³¼ ë™ì¼í•©ë‹ˆë‹¤.

$$
\begin{align}
V(G,D^*) &= E_{x \sim p_{data} (x)} [ \log(D^{*}(x)) ] + E_{x \sim p_{g} (x)} [ \log(1-D^{*}(x)) ]\\
&= - \log(4) + \text{KL} (p_{data}  \Vert  \frac{p_{data} + p_{g}}{2}) + \text{KL} (p_{g}  \Vert  \frac{p_{data} + p_{g}}{2})\\
&= - \log(4) + 2 \times \text{JSD}(p_{data}  \Vert  p_{g})

\end{align}

$$

## WGAN

 ê¸°ì¡´ì˜ GAN ì²˜ëŸ¼ Jensen-Shannon Divergence ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ì€ vanishing gradientsê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  generatorì˜ íŒŒë¼ë¯¸í„°ì— ì˜í•´ ì ì¬ì ìœ¼ë¡œ ë¶ˆì—°ì†ì ì¸ divergence($p_g$ ì˜ supportê°€ $p_{data}$ ì™€ ì „í˜€ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê²½ìš°)ì´ ë  ìˆ˜ ìˆëŠ” Jensen-Shannon Divergence ëŒ€ì‹ ì—  WGANì—ì„œëŠ” ëŒ€ì•ˆìœ¼ë¡œ Wasserstein-1 distanceë¥¼ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤.

### **Wasserstein-1 distance**

 Earth-Moverë¼ê³ ë„ ë¶ˆë¦¬ëŠ” Wasserstein-1 distanceëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ ë©ë‹ˆë‹¤. 

$$
W(p_r,p_g)=\inf_{\gamma \sim \Pi(p_r,p_g)}\mathbb{E}_{(x,y) \sim \gamma}[ \Vert x-y \Vert ]
$$

ì¦‰, $p_r$ ë¥¼ $p_{g}$ ë¡œ ë³€í˜•í•˜ê¸° ìœ„í•œ transporting massì˜ ìµœì†Œ ë¹„ìš©(í•˜í•œ)ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤.

Kantorovich-Rubenstein dualityë¥¼ ì´ìš©í•˜ì—¬ Wasserstein-1 distanceë¥¼ value functionë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
\min_G\max_{D\in \mathcal{D}}\mathbb{E}_{x \sim \mathbb{P}_r}[D(x)]-\mathbb{E}_{\tilde x \sim \mathbb{P}_g}[D(\tilde x)]
$$

$\mathcal{D}$ ëŠ” 1-Lipschitz functionsì´ë©° $\mathbb{P}_g$ ëŠ” $\tilde x \sim G(z), z \sim p(z)$ ë¡œ ì •ì˜ë©ë‹ˆë‹¤. (GANì€ í™•ë¥ ë¶„í¬ $p$ ì— ëŒ€í•´ ì œì•½ì´ ì—†ì–´ì„œ ë¬´ì—‡ì´ë“  ìƒê´€ì—†ë‹¤) optimal Discriminatorê°€ ì£¼ì–´ì¡Œì„ ë•Œ WGANì˜ value functionì„ generatorì— ëŒ€í•´ ìµœì†Œí™”í•˜ëŠ” ê²ƒì€ $W\left(\mathbb{P}{r}, \mathbb{P}{g}\right)$ ë¥¼ ìµœì†Œí™” í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. 

WGANì˜ value functionì„ ì‚¬ìš©í•  ë•Œ Discriminatorê°€ 1-Lipschitz functions ì´ì–´ì•¼ í•œë‹¤ëŠ” ì¡°ê±´ì´ ìƒê²¼ë‹¤. WGANì—ì„œëŠ” Discriminator(WGANì—ì„œëŠ” criticì´ë¼ê³  ë¶€ë¥¸ë‹¤)ì˜ weights setì´ [-c,c]ë²”ìœ„ë¥¼ ê°–ë„ë¡ ì œí•œí•˜ì—¬ ì´ ì¡°ê±´ì„ ë§Œì¡± ì‹œì¼°ë‹¤. cëŠ” criticì˜ architectureì— ì˜ì¡´í•©ë‹ˆë‹¤. 

ì‹¤ì œë¡œ ì•„ë˜ì™€ ê°™ì´ êµ¬í˜„ ë©ë‹ˆë‹¤. cëŠ” 0.01ì¸ë° ì‹¤í—˜ì„ í†µí•´ êµ¬í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. 

![Untitled](/assets/WGAN-GP_img/Untitled%201.png)

# **Abstract**

WGAN-GPì—ì„œëŠ” WGANì—ì„œ 1-Lipschitz constraintë¥¼ ë§Œì¡±ì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©í•œ weight clippingì´ ì¢‹ì§€ ì•ŠìŒì„ ë³´ì´ê³  **lossì— gradient penalty** ë¶€ê³¼í•˜ì—¬ 1-Lipschitz constraintë¥¼ ë§Œì¡±ì‹œí‚¤ëŠ” ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. 

# Contributions

ë…¼ë¬¸ì˜ ContributionsëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. Toy datasetsì— ëŒ€í•´ criticì˜ weight clippingì´ undesired behaviorë¥¼ ìœ ë°œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤.
2. â€œGradient penaltyâ€(WGAN-GP)ë¥¼ ì œì•ˆí•˜ê³  ì´ë¥¼ ì´ìš©í•˜ì—¬ weight clippingë¡œ ì¸í•œ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.
3. ë‹¤ì–‘í•œ GAN êµ¬ì¡°ì— ëŒ€í•´ ì•ˆì •ì ì¸ í•™ìŠµì„ ì¦ëª…í•˜ê³ , weigth clippingì— ëŒ€í•œ ì„±ëŠ¥ í–¥ìƒ, ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±, ê°œë³„ ìƒ˜í”Œë§ì´ ì—†ì´ ë¬¸ì ìˆ˜ì¤€ì˜ GAN ì–¸ì–´ëª¨ë¸ì„ ì„ ë³´ì¸ë‹¤.

# Properties of the optimal WGAN critic

weight clippingì˜ ë¬¸ì œì ì„ ë°íˆê³  gradient penaltyì˜ í•„ìš”ì„±ì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ optimal WGAN criticì— ëŒ€í•´ ì§šê³  ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤. 

![Untitled](/assets/WGAN-GP_img/Untitled%202.png)

![Untitled](/assets/WGAN-GP_img/Untitled%203.png)

<aside>

compact metric space $\mathcal{X}$ ìƒì˜ ë‘ ë¶„í¬  $\mathbb{P}_{r}$ ì™€ $\mathbb{P}_{g}$ ê°€ ìˆì„ ë•Œ,1-Lipschitz function $f^*$ ê°€  $\max_{ \Vert f \Vert _L \leq 1 }\mathbb{E}_{y \sim \mathbb{P}_r}[f(y)]-\mathbb{E}_{x \sim \mathbb{P}_g}[f( x)]$ ì˜ optimal solution ì´ë¼ê³  í•˜ì.$\Pi(\mathbb{P}_r,\mathbb{P}_g)$ ì„ ë‘ ë¶„í¬ì˜ ê²°í•©ë¶„í¬ë“¤ì˜ ì§‘í•©ì´ë¼ê³  ë‘ê³  $W(\mathbb{P}_r,\mathbb{P}_g)=\inf_{\pi \sim \Pi(\mathbb{P}_r,\mathbb{P}_g)}\mathbb{E}_{(x,y) \sim \pi}[ \Vert x-y \Vert ]$ ì˜ optimal coupling $\pi$ ì´ ìˆë‹¤ê³  í•  ë•Œ, 

> $f^*$ ê°€ ë¯¸ë¶„ê°€ëŠ¥í•˜ê³  $\pi(x=y)=0$ ì„ ë§Œì¡±í•˜ë©´ $x_t=tx+(1-t)y \text{ with } 0 \leq t \leq 1$ ì¼ë•Œ $\mathbb{P}_{(x,y) \sim \pi}[\nabla f^*(x_t)=\frac{y-x_t}{ \Vert y-x_t \Vert }]=1$ ë¥¼ ë§Œì¡±í•œë‹¤.
> 

ë”°ë¦„ì •ë¦¬:  $\mathbb{P}_{r}$, $\mathbb{P}_{g}$ ì—ì„œ $f^*$ ì˜ gradientì˜ í¬ê¸°ëŠ” ê±°ì˜ ëŒ€ë¶€ë¶„ 1ì´ë‹¤. 

</aside>

<aside>

ğŸ’¡ ê±°ì°½í•´ ë³´ì´ì§€ë§Œ ì£¼ëª©í• ë§Œí•œ ë¶€ë¶„ì€ $\mathbb{P}_{r}$, $\mathbb{P}_{g}$ ê°€ ê²¹ì¹˜ëŠ” êµ¬ê°„ì—ì„œ ë½‘ì€ ë‘ ë°ì´í„°ì˜ ì‚¬ì´ì— ìˆëŠ” ë°ì´í„°ì— ëŒ€í•´ **optimal WGAN critic**ì¸ 1-Lipschitz function $f^*$ ì˜ gradientê°€ **1**ì´ë¼ëŠ” ì ì…ë‹ˆë‹¤. 1-Lipschitz functionì˜ ì •ì˜ëŠ” $ \vert f(x_1)-f(x_2) \vert \leq 1* \vert x_1-x_2 \vert $ ì¸ ë§Œí¼ $f^*$ ì˜ gradientëŠ”$-1\leq f'(x) \leq 1$ ë¥¼ ê°€ì ¸ë„ ë˜ëŠ”ë° optimal solution $f^*$ ëŠ” ê±°ì˜ í•­ìƒ gradientê°€ 1ì´ë¼ëŠ” ê²ƒì´ì£ .  
ê·¸ë¦¬ê³  ë”°ë¦„ ì •ë¦¬ì—ì„œëŠ”  $\mathbb{P}_{r}$, $\mathbb{P}_{g}$ ì—ì„œ $f^*$ ì˜ gradientê°€ ê±°ì˜ ì–´ë””ì„œë‚˜(almost everywhere) 1ì´ë¼ëŠ” ê²ƒì„ ë°í™ë‹ˆë‹¤.

</aside>

ë’¤ì—ì„œ ì´ëŸ¬í•œ íŠ¹ì§•ì„ ì´ìš©í•˜ì—¬ Gradient penaltyë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

# Difficulties with weight constraints

![Untitled](/assets/WGAN-GP_img/Untitled%204.png)

ê°€ì¤‘ì¹˜ì— ì œì•½ì¡°ê±´ì„ ë‹¤ëŠ” ê²ƒì´ í•™ìŠµì— ì¢‹ì§€ ì•Šì€ ì˜í–¥ì„ ë¼ì¹œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ëŠ” ì‹¤í—˜ì…ë‹ˆë‹¤. ì™¼ìª½ì˜ ì‹¤í—˜ì€ WGANì— weight clippingì„ ì ìš©í•œ ê²ƒ(ìœ„) ê³¼ Gradient penaltyë¥¼ ì ìš©í•œ ê²ƒ(ì•„ë˜)ë¥¼ ë¹„êµí•œ ì‹¤í—˜ì…ë‹ˆë‹¤. 

# Capacity underuse

weight clippingì„ ì ìš©í•˜ì—¬ k-Lipshitz constraintë¥¼ êµ¬í˜„í•˜ë©´ criticsê°€ ë‹¨ìˆœí•œ í•¨ìˆ˜ë¡œ í¸í–¥ëœë‹¤ê³  í•©ë‹ˆë‹¤. Figure1ì˜ ê·¸ë¦¼ aëŠ” ìµœì ì˜ clipping rangeë¥¼ ì°¾ì•„ í•™ìŠµì‹œí‚¨ ê²ƒì„ì—ë„ ë¶ˆêµ¬í•˜ê³  Gradient penaltyì— ë¹„í•´ ìƒë‹¹íˆ ë‹¨ìˆœí•œ value surfaceë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. 

# Gradient penalty

<aside>
âœ… Wasserstein-1 distanceë¥¼  ì´ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” criticì´ 1-Lipshitz functionì´ì–´ì•¼ í•©ë‹ˆë‹¤. WGANì—ì„œëŠ” weight clippingì„ ì´ìš©í•˜ì—¬ ì´ëŸ¬í•œ ì¡°ê±´ì„ ì¶©ì¡±ì‹œì¼°ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ìœ„ì—ì„œ ë³´ì¸ ê²ƒì²˜ëŸ¼ ì´ëŸ¬í•œ ë°©ë²•ì€ ëª¨ë¸ì— ì¢‹ì§€ ì•Šì€ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</aside>

WGAN-GPëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤. 1-Lipshitz functionë…¼ë¬¸ì—ì„œëŠ” ì§ì ‘ì ìœ¼ë¡œ criticì˜ gradient í¬ê¸°(norm)ì— ëŒ€í•œ ì œì•½ì¡°ê±´ì„ lossí•¨ìˆ˜ì— ë°˜ì˜í•©ë‹ˆë‹¤.

í•­ìƒ ë¯¸ë¶„ê°€ëŠ¥ í•¨ìˆ˜ì˜ gradientì˜ í¬ê¸°(norm)ê°€ ê±°ì˜ ì–´ë””ì„œë‚˜(almost everywhere) 1ì´ë¼ëŠ” ê²ƒì€ 1-Lipshitz functionì´ê¸° ìœ„í•œ í•„ìš”ì¶©ë¶„ì¡°ê±´ì…ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ ì¡°ê±´ì„ soft versionìœ¼ë¡œ ë¶€ì—¬í•˜ê¸° ìœ„í•´ $\hat{x}\sim\mathbb{P}_{\hat{x}}$ ì—ì„œ samplingí•˜ì—¬ penaltyë¡œ ë¶€ì—¬í•©ë‹ˆë‹¤. 

$$
L=\underbrace{\mathbb{E}_{\tilde x \sim \mathbb{P}_g}[D(\tilde x)]-\mathbb{E}_{x \sim \mathbb{P}_r}[D(x)]}_{\text{WGANì˜ critic loss}}+\underbrace{\lambda\mathbb{E}_{\hat{x}\sim\mathbb{P}_{\hat{x}}}[( \Vert \nabla_{\hat{x}} D(\hat{x}) \Vert _2-1)^2]}_{\text{ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” gradient penalty}}
$$

Gradient penaltyëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬í˜„ ë©ë‹ˆë‹¤.

```python
def _gradient_penalty(self, real_data, generated_data):
	batch_size = real_data.size()[0]

  # Calculate interpolation
  alpha = torch.rand(batch_size, 1, 1, 1)
  alpha = alpha.expand_as(real_data)
  if self.use_cuda:
      alpha = alpha.cuda()
  interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data
  interpolated = Variable(interpolated, requires_grad=True)
  if self.use_cuda:
      interpolated = interpolated.cuda()
  # Calculate probability of interpolated examples
  prob_interpolated = self.D(interpolated)
  # Calculate gradients of probabilities with respect to examples
  gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,
                         grad_outputs=torch.ones(prob_interpolated.size()).cuda() if self.use_cuda else torch.ones(
                         prob_interpolated.size()),
                         create_graph=True, retain_graph=True)[0]
  # Gradients have shape (batch_size, num_channels, img_width, img_height),
  # so flatten to easily take norm per example in batch
  gradients = gradients.view(batch_size, -1)
  self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().data[0])
  # Derivatives of the gradient close to 0 can cause problems because of
  # the square root, so manually calculate norm and add epsilon
  gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)
  # Return gradient penalty
  return self.gp_weight * ((gradients_norm - 1) ** 2).mean()
```

### Sampling distribution

 $\hat{x} \sim \mathbb{P}_{ \hat{x} }$ ì—ì„œ $\mathbb{P}_{\hat{x}}$ ëŠ” real imageë¶„í¬, $\mathbb{P}_r$ ì™€ generated image ë¶„í¬, $\mathbb{P}_g$ ì˜ ì‚¬ì´ì—ì„œ ì„ í˜• ë³´ê°„ì„ í†µí•´ êµ¬í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ì€ Properties of the optimal WGAN criticì—ì„œ ì´ì•¼ê¸°í–ˆë˜ ë°ë¡œ ë‘ ë¶„í¬ì˜ ì‚¬ì´ì˜ ë°ì´í„°ì— ëŒ€í•´ criticì˜ gardientì˜ í¬ê¸°ê°€ 1ì„ ê°–ëŠ”ë‹¤ëŠ” ì‚¬ì‹¤ì—ì„œ ê¸°ë°˜í•©ë‹ˆë‹¤. ëª¨ë“  ê³³ì—ì„œ gradient norm ì œì•½ì„ ì£¼ëŠ” ê²ƒì€ ì–´ë µê¸° ë•Œë¬¸ì—, ì´ëŸ¬í•œ ì§ì„ ì„ ë”°ë¼ ì‹œí–‰í•˜ëŠ” ê²ƒìœ¼ë¡œë„ ì¶©ë¶„í•˜ê³  ì‹¤í—˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.

### Penalty coefficient

ì‹¤í—˜ì„ í†µí•´ $\lambda=10$ ì´ ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì˜ ì‘ë™í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

### No critic batch normalization

ì•ì„  ì„ í–‰ GAN êµ¬í˜„ë“¤ì—ì„œëŠ” generator & discriminator ëª¨ë‘ batch normalizationì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì•ˆì •í™” ì‹œí‚¤ëŠ”ë° ë„ì›€ì„ ì£¼ë ¤í–ˆì§€ë§Œ, batch normalizationì€ discriminatorì˜ ë‹¨ì¼ ì…ë ¥ì„ ë‹¨ì¼ ì¶œë ¥ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë¬¸ì œë¡œë¶€í„°, ì…ë ¥ì˜ ì „ì²´ ë°°ì¹˜ë¡œë¶€í„° ì¶œë ¥ì˜ ë°°ì¹˜ë¡œ ë§¤í•‘í•˜ëŠ” ë¬¸ì œë¡œ ìœ í˜•ì„ ë³€í™”ì‹œí‚¨ë‹¤. ê¸°ì¡´ì— ì „ì²´ ë°°ì¹˜ê°€ ì•„ë‹ˆë¼ ê° ì…ë ¥ì— ë…ë¦½ì ìœ¼ë¡œ criticì˜ gradient normì„ ì²˜ë²Œí•˜ê¸° ë•Œë¬¸ì—, ë…¼ë¬¸ì˜ íŒ¨ë„í‹°ë¥¼ ì£¼ëŠ” í•™ìŠµì€ ì´ëŸ¬í•œ í™˜ê²½ì—ì„œ ë” ì´ìƒ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ, ê°„ë‹¨í•˜ê²Œ ëª¨ë¸ ë‚´ì˜ criticì—ì„œ batch normalizationì„ ìƒëµí•œë‹¤. batch normalization ëŒ€ì²´ë¡œ layer normalizationì„ ì¶”ì²œí•œë‹¤.

### Two-sided penalty

ë…¼ë¬¸ì—ì„œëŠ” gradientì˜ normì´ 1 ì•„ë˜ì— ë¨¸ë¬´ë¥´ê¸°(one-sided penalty) ë³´ë‹¤ëŠ” 1ë¡œ í–¥í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤.(two-sided penalty). 1ì´ë¼ëŠ” íŠ¹ì •ê°’ìœ¼ë¡œ ì œì•½ì„ ê±¸ì–´ë„ ì‹¤ì¦ì ìœ¼ë¡œ ë´¤ì„ ë•Œ ì´ëŸ¬í•œ penaltyê°€ criticì„ ë§ì´ ê·œì œí•˜ì§€ ì•ŠëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ë¡œëŠ” ìµœì ì˜ WGAN criticì€   $\mathbb{P}_{r}$, $\mathbb{P}_{g}$ í•˜ì˜ ê±°ì˜ ëª¨ë“  ê³³ì—ì„œ gradient norm 1ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì¼ ê²ƒì´ë¼ê³  ë³´ì•˜ìŠµë‹ˆë‹¤. 

![Untitled](/assets/WGAN-GP_img/Untitled%205.png)

# Experiments

## Training random architectures within a set

DCGANì˜ ê¸°ë³¸ êµ¬ì¡°ì—ì„œ ì•½ê°„ì˜ ìˆ˜ì •ì„ í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ëª¨ë¸ì„ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

![Untitled](/assets/WGAN-GP_img/Untitled%206.png)

ì´ëŸ¬í•œ êµ¬ì„±ì—ì„œ 32*32 ImageNetìœ¼ë¡œ í•™ìŠµì‹œì¼°ê³  WGAN-GP, standard GAN objectivesë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  min_scoreë¥¼ ì •í•˜ê³  ìƒì„±í•œ ì´ë¯¸ì§€ì˜ inception_scoreê°€ min_scoreë¥¼ ë„˜ìœ¼ë©´ ì„±ê³µìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. inception_scoreëŠ” ë†’ì„ ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤.

![Untitled](/assets/WGAN-GP_img/Untitled%207.png)

WGAN-GPì˜ ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Training varied architectures on LSUN bedrooms

![Untitled](/assets/WGAN-GP_img/Untitled%208.png)

WGAN-GP lossë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµì´ ì˜ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Improved performance over weight clipping

![Untitled](/assets/WGAN-GP_img/Untitled%209.png)

ì™¼ìª½ì€ iterationì— ë”°ë¥¸ Inception Scoreì´ê³  ì˜¤ë¥¸ìª½ì€ ì‹œê°„ì— ë”°ë¥¸ Inception Scoreì…ë‹ˆë‹¤. WGAN-GPëŠ” weight clippingë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Modeling discrete data with a continuous generator

ëª¨ë¸ì˜ degenerate distributionsì„ ëª¨ë¸ë§ í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë³´ì´ê¸° ìœ„í•˜ì—¬ ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë³µì¡í•œ ì´ì‚° í™•ë¥  ë¶„í¬ë¥¼ ëª¨ë¸ë§ í•˜ë„ë¡ í›ˆë ¨ ì‹œí‚µë‹ˆë‹¤. Google Billion Word datasetì„ ì‚¬ìš©í•˜ê³  1D CNNì™€ softmaxë¥¼ ëª¨ë¸ì— ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. 

![Untitled](/assets/WGAN-GP_img/Untitled%2010.png)

ëª¨ë¸ì€ ë³„ë„ì˜ ìƒ˜í”Œë§ ì—†ì´ latent vectorë¡œ ë¶€í„° ì§ì ‘ one-hot character embeddingí•˜ëŠ” ê²ƒì„ í•™ìŠµí•œë‹¤. ê¸°ì¡´ì˜ GANê³¼ëŠ” ë¹„êµí•  ë§Œí•œ ê²°ê³¼ë¥¼ ì–»ì§€ëŠ” ëª»í–ˆìŠµë‹ˆë‹¤.

# Conclusion

WGANì—ì„œ weight clippingì˜ ë¬¸ì œë¥¼ í™•ì¸í•˜ê³  ë™ì¼í•œ ë¬¸ì œë¥¼ ë‚˜íƒ€ë‚´ì§€ ì•ŠëŠ” Gradient penaltyë¥¼ í†µí•œ ê°œì„ ì•ˆì„ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ GANì„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ì•ˆì„ ë§ˆë ¨í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ GANì„ í›ˆë ¨í•˜ê¸° ìœ„í•œ ë³´ë‹¤ ì•ˆì •ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•˜ì—¬ ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì´ìš©í•œ ë” ë³µì¡í•œ ëª¨ë¸ì˜ ê°€ëŠ¥ì„±ì„ ì—´ì—ˆë‹¤ëŠ” ì˜ë¯¸ê°€ ìˆë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ì¥ë‹¨ì 

ì¥ì : 

1. WGANì˜ ë¬¸ì œë¥¼ ì˜ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤. WGAN-GPì˜ íŒŒê¸‰ë ¥ì€ êµ‰ì¥íˆ í½ë‹ˆë‹¤. StyleGANê³¼ PGGANê³¼ ê°™ì€ ê·œëª¨ê°€ í¬ê³  ë³µì¡í•œ GANì„ í•™ìŠµì‹œí‚¤ëŠ”ë° ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. GANì„ í•™ìŠµì‹œí‚¤ê³ ì í• ë•Œ WGAN-GPë¥¼ ì‚¬ìš©í•˜ë©´ ìƒë‹¹íˆ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¨ì :

1. discrete data with a continuous generatorì˜ ê²°ê³¼ëŠ” ë‹¤ì†Œ ì˜ë¬¸ì´ ë“­ë‹ˆë‹¤. ë‹¤ë¥¸ text generatorì™€ì˜ ë¹„êµë‚˜ WGAN-GPë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì´ ì´ë¤„ì§€ì§€ ì•Šì€ ê²ƒì„ ë³´ì—¬ ì£¼ì—ˆìœ¼ë©´ ì¢‹ì•˜ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

# ì˜ë¯¸

WGAN-GP lossëŠ” í•™ìŠµì„ ìƒë‹¹íˆ ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ê¸° ë•Œë¬¸ì— PGGANë¶€í„° ì‹œì‘í•˜ì—¬ ëŒ€ë¶€ë¶„ì˜ GANëª¨ë¸ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” lossí•¨ìˆ˜ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. 

![Untitled](/assets/WGAN-GP_img/Untitled%2011.png)

PGGANì€ WGAN-GPë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ 1024*1024 í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

# í™œìš©

WGAN-GPëŠ” ì´ë¯¸ì§€ í•©ì„± ì™¸ì— ìŒì„± í•©ì„±, ì¶”ì²œ ì‹œìŠ¤í…œì—ë„ í™œìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

ìŒì„± í•©ì„±: ë‹¤í™”ì ìŒì„±í•©ì„±

[Wasserstein GAN and Waveform Loss-based Acoustic Model Training for Multi-speaker Text-to-Speech Synthesis Systems Using a WaveNet Vocoder](https://arxiv.org/abs/1807.11679)

ì¶”ì²œ ì‹œìŠ¤í…œ:

[Application of WGAN-GP in recommendation and Questioning the relevance of GAN-based approaches](https://arxiv.org/abs/2204.12527)

ì°¸ê³ ë¬¸í—Œ

[https://arxiv.org/pdf/1704.00028.pdf](https://arxiv.org/pdf/1704.00028.pdf)

[https://ysbsb.github.io/gan/2022/02/18/WGAN-GP.html](https://ysbsb.github.io/gan/2022/02/18/WGAN-GP.html)

[https://haawron.tistory.com/21](https://haawron.tistory.com/21)

[https://courses.cs.washington.edu/courses/cse599i/20au/resources/L12_duality.pdf](https://courses.cs.washington.edu/courses/cse599i/20au/resources/L12_duality.pdf)

[https://arxiv.org/pdf/1904.08994.pdf](https://arxiv.org/pdf/1904.08994.pdf)

[https://leechamin.tistory.com/232#---%--Training%--random%--architectures%--within%--a%--set](https://leechamin.tistory.com/232#---%25--Training%25--random%25--architectures%25--within%25--a%25--set)

[https://en.wikipedia.org/wiki/Degenerate_distribution](https://en.wikipedia.org/wiki/Degenerate_distribution)

[https://github.com/EmilienDupont/wgan-gp/blob/master/training.py](https://github.com/EmilienDupont/wgan-gp/blob/master/training.py)